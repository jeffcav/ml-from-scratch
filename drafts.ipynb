{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "WEIGHTS=0\n",
    "ACTIVATION=1\n",
    "\n",
    "class GradientDescent:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, model, inputs, outputs, lr, epochs):\n",
    "        \"\"\" Runs the Grandient Descent algorithm and modifies model during trainning\n",
    "\n",
    "        Parameters:\n",
    "            model (array): array of tuples, each tuple contains an array of \n",
    "                initialized weights and an activation function. The idea is\n",
    "                to extend it to models with multiple layers later (eg: MLP).\n",
    "                Examples: \n",
    "                    f(x) = w0 + w1x -> [([w0, w1], Identity())]\n",
    "                    g(x) = sigmoid(w0 + w1x) -> [([w0, w1], Sigmoid())]\n",
    "            \n",
    "            inputs (numpy.Array): array of inputs with potentially multiple features\n",
    "            \n",
    "            outputs (numpy.Array): array of outputs for each input\n",
    "            \n",
    "            lr (float): learning rate\n",
    "            \n",
    "            epochs (int): number of epochs\n",
    "\n",
    "        Returns:\n",
    "            errors (array): trainning errors at each epoch\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # extract model's weights and activation funtion\n",
    "        layer_idx = 0\n",
    "        weights = model[layer_idx][WEIGHTS]\n",
    "        activation = model[layer_idx][ACTIVATION]\n",
    "\n",
    "        # Add column of ones to the input\n",
    "        num_samples = inputs.shape[0]\n",
    "        inputs = np.c_[np.ones(num_samples), inputs]\n",
    "\n",
    "        errors = []\n",
    "        for _ in range(epochs):\n",
    "            predictions = activation(inputs @ weights.T)\n",
    "            error = outputs - predictions\n",
    "            \n",
    "            gradients = (inputs * error).mean(axis=0)\n",
    "            weights += lr * gradients\n",
    "\n",
    "            errors.append(error)\n",
    "            \n",
    "        return errors\n",
    "\n",
    "class StochasticGradientDescent:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, model, inputs, outputs, lr, epochs):\n",
    "        \"\"\" Runs the Grandient Descent algorithm and modifies model during trainning\n",
    "\n",
    "        Parameters:\n",
    "            model (array): array of tuples, each tuple contains an array of \n",
    "                initialized weights and an activation function. The idea is\n",
    "                to extend it to models with multiple layers later (eg: MLP).\n",
    "                Examples: \n",
    "                    f(x) = w0 + w1x -> [([w0, w1], Identity())]\n",
    "                    g(x) = sigmoid(w0 + w1x) -> [([w0, w1], Sigmoid())]\n",
    "            \n",
    "            inputs (numpy.Array): array of inputs with potentially multiple features\n",
    "            \n",
    "            outputs (numpy.Array): array of outputs for each input\n",
    "            \n",
    "            lr (float): learning rate\n",
    "            \n",
    "            epochs (int): number of epochs\n",
    "\n",
    "        Returns:\n",
    "            errors (array): trainning errors at each epoch\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # extract model's weights and activation funtion\n",
    "        layer_idx = 0\n",
    "        weights = model[layer_idx][WEIGHTS]\n",
    "        activation = model[layer_idx][ACTIVATION]\n",
    "\n",
    "        # Add column of ones to the input\n",
    "        num_samples = inputs.shape[0]\n",
    "        inputs = np.c_[np.ones(num_samples), inputs]\n",
    "\n",
    "        errors = []\n",
    "        for _ in range(epochs):\n",
    "            # shuffle data\n",
    "            shuffle = np.random.permutation(num_samples)\n",
    "            inputs = inputs[shuffle]\n",
    "            outputs = outputs[shuffle]\n",
    "\n",
    "            for i in range(num_samples):\n",
    "                prediction = activation(inputs[i] @ weights.T)\n",
    "                error = outputs[i] - prediction\n",
    "\n",
    "                gradients = (inputs[i] * error)\n",
    "                weights += (lr * gradients)\n",
    "\n",
    "            # store errors\n",
    "            errors.append(error)\n",
    "\n",
    "        return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 3.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ml.functions as fc\n",
    "\n",
    "weights = np.array([\n",
    "    [0.0, 0.0]\n",
    "])\n",
    "parameters = (weights, fc.Identity())\n",
    "\n",
    "weights_expected = np.array([2.0, 3.0])\n",
    "\n",
    "x = np.array([\n",
    "    [3.0], \n",
    "    [6.0], \n",
    "    [7.0]\n",
    "])\n",
    "y = np.array([\n",
    "    [11.0], \n",
    "    [20.0], \n",
    "    [23.0]\n",
    "])\n",
    "\n",
    "epochs = 2000\n",
    "alpha = 0.05\n",
    "sgd = StochasticGradientDescent()\n",
    "errors = sgd([parameters], x, y, alpha, epochs)\n",
    "\n",
    "print(parameters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ml.functions as fc\n",
    "import ml.algorithms.optimizers as optimizers\n",
    "\n",
    "weights = np.array([\n",
    "    [0.0, 0.0]\n",
    "])\n",
    "parameters = (weights, fc.Identity())\n",
    "\n",
    "weights_expected = np.array([2.0, 3.0])\n",
    "\n",
    "x = np.array([\n",
    "    [3.0], \n",
    "    [6.0], \n",
    "    [7.0]\n",
    "])\n",
    "y = np.array([\n",
    "    [11.0], \n",
    "    [20.0], \n",
    "    [23.0]\n",
    "])\n",
    "\n",
    "epochs = 5000\n",
    "alpha = 0.05\n",
    "solver = optim.GradientDescent()\n",
    "errors = solver([parameters], x, y, alpha, epochs)\n",
    "\n",
    "print(parameters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jeff/dev/ml-from-scratch/drafts.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeff/dev/ml-from-scratch/drafts.ipynb#ch0000006?line=20'>21</a>\u001b[0m gd \u001b[39m=\u001b[39m GradientDescent()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeff/dev/ml-from-scratch/drafts.ipynb#ch0000006?line=22'>23</a>\u001b[0m lin \u001b[39m=\u001b[39m LinearRegression()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jeff/dev/ml-from-scratch/drafts.ipynb#ch0000006?line=23'>24</a>\u001b[0m lin\u001b[39m.\u001b[39;49mfit(x, y, gd, epochs, alpha, gamma)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jeff/dev/ml-from-scratch/drafts.ipynb#ch0000006?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(lin\u001b[39m.\u001b[39mparams)\n",
      "File \u001b[0;32m~/dev/ml-from-scratch/ml/models/linear.py:15\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, inputs, outputs, optimizer, epochs, learning_rate, regularization)\u001b[0m\n\u001b[1;32m     <a href='file:///home/jeff/dev/ml-from-scratch/ml/models/linear.py?line=11'>12</a>\u001b[0m weights \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(num_params)\n\u001b[1;32m     <a href='file:///home/jeff/dev/ml-from-scratch/ml/models/linear.py?line=13'>14</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams \u001b[39m=\u001b[39m [(weights, functions\u001b[39m.\u001b[39mIdentity)]\n\u001b[0;32m---> <a href='file:///home/jeff/dev/ml-from-scratch/ml/models/linear.py?line=14'>15</a>\u001b[0m optimizer(\u001b[39mself\u001b[39;49m, inputs, outputs, epochs, learning_rate, regularization)\n",
      "File \u001b[0;32m~/dev/ml-from-scratch/ml/algorithms/optimizers.py:46\u001b[0m, in \u001b[0;36mGradientDescent.__call__\u001b[0;34m(self, model, inputs, outputs, epochs, lr, regularization)\u001b[0m\n\u001b[1;32m     <a href='file:///home/jeff/dev/ml-from-scratch/ml/algorithms/optimizers.py?line=42'>43</a>\u001b[0m errors \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='file:///home/jeff/dev/ml-from-scratch/ml/algorithms/optimizers.py?line=43'>44</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     <a href='file:///home/jeff/dev/ml-from-scratch/ml/algorithms/optimizers.py?line=44'>45</a>\u001b[0m     \u001b[39m# compute estimates\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/jeff/dev/ml-from-scratch/ml/algorithms/optimizers.py?line=45'>46</a>\u001b[0m     predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(inputs)\n\u001b[1;32m     <a href='file:///home/jeff/dev/ml-from-scratch/ml/algorithms/optimizers.py?line=46'>47</a>\u001b[0m     error \u001b[39m=\u001b[39m outputs \u001b[39m-\u001b[39m predictions\n\u001b[1;32m     <a href='file:///home/jeff/dev/ml-from-scratch/ml/algorithms/optimizers.py?line=48'>49</a>\u001b[0m     \u001b[39m# update weights\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/jeff/dev/ml-from-scratch/ml/algorithms/optimizers.py?line=49'>50</a>\u001b[0m     \u001b[39m# TODO support gradient of activation functions\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/ml-from-scratch/ml/models/linear.py:18\u001b[0m, in \u001b[0;36mLinearRegression.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='file:///home/jeff/dev/ml-from-scratch/ml/models/linear.py?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='file:///home/jeff/dev/ml-from-scratch/ml/models/linear.py?line=17'>18</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m] \u001b[39m@\u001b[39;49m x\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 2)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ml.algorithms.optimizers import GradientDescent\n",
    "from ml.models.linear import LinearRegression\n",
    "\n",
    "weights_expected = np.array([2.0, 3.0])\n",
    "\n",
    "x = np.array([\n",
    "    [3.0], \n",
    "    [6.0], \n",
    "    [7.0]\n",
    "])\n",
    "y = np.array([\n",
    "    [11.0], \n",
    "    [20.0], \n",
    "    [23.0]\n",
    "])\n",
    "\n",
    "epochs = 5000\n",
    "alpha = 0.05\n",
    "gamma = 0\n",
    "gd = GradientDescent()\n",
    "\n",
    "lin = LinearRegression()\n",
    "lin.fit(x, y, gd, epochs, alpha, gamma)\n",
    "\n",
    "print(lin.params)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
